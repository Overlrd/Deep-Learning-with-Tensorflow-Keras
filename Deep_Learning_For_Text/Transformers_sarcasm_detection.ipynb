{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf   \n",
        "from tensorflow import keras \n",
        "from keras import layers\n",
        "from keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qyOs2mWRQxfD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A-eWORcpWRC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "The purpose of self-attention is to modulate the representation of a token\n",
        "by using the representations of related tokens in the sequence. This produces \n",
        "context aware token representations.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCga9ykZPwGf",
        "outputId": "4731d7c1-324c-4d49-a803-ed3e5aa0be4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 16:32:55--  https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.114.128, 108.177.111.128, 142.250.1.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.114.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: ‘sarcasm.json’\n",
            "\n",
            "\rsarcasm.json          0%[                    ]       0  --.-KB/s               \rsarcasm.json        100%[===================>]   5.38M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-04-05 16:32:55 (60.0 MB/s) - ‘sarcasm.json’ saved [5643545/5643545]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "\n",
        "with open('sarcasm.json', 'r') as f :\n",
        "  datastore = json.load(f)\n",
        "\n",
        "  labels = []\n",
        "sentences = []\n",
        "\n",
        "for item in datastore:\n",
        "  sentences.append(item['headline'])\n",
        "  labels.append(item['is_sarcastic'])\n",
        "\n",
        "print(\"sentences len\",  len(sentences))\n",
        "print(\"labels len\", len(labels))\n",
        "print(\"sentences[0]\", sentences[0])  \n",
        "print(\"labels[0]\", labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAruJja7Pyw2",
        "outputId": "01cbcabe-ee79-44b9-a69f-a326abd26c5c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentences len 26709\n",
            "labels len 26709\n",
            "sentences[0] former versace store clerk sues over secret 'black code' for minority shoppers\n",
            "labels[0] 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = np.array(sentences)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "WYQzSGTyPyrV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(features , labels):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "  ds = ds.shuffle(buffer_size=len(features))\n",
        "  ds = ds.batch(batch_size=32)\n",
        "  return ds "
      ],
      "metadata": {
        "id": "etItW90zPyn4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences , test_sentences , train_labels , test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42 )\n",
        "train_sentences , val_sentences , train_labels, val_labels = train_test_split(train_sentences , train_labels , test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "_TCldtjjQEqg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_sentences , train_labels)\n",
        "val_ds = make_dataset(val_sentences , val_labels)\n",
        "test_ds = make_dataset(test_sentences , test_labels)"
      ],
      "metadata": {
        "id": "nnFSkJpxQEoN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n"
      ],
      "metadata": {
        "id": "vZYvxSAOQEl6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing integer sequence datasets\n",
        "max_length = 250\n",
        "max_tokens = 10000\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens = max_tokens ,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = max_length,\n",
        ")"
      ],
      "metadata": {
        "id": "LIyr_QLXQEiS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y),\n",
        "num_parallel_calls=4)\n",
        "\n",
        "int_val_ds = val_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y),\n",
        "num_parallel_calls=4)\n",
        "\n",
        "int_test_ds = test_ds.map(\n",
        "lambda x, y: (text_vectorization(x), y),\n",
        "num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "PAam3Zu5QZ4K"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer encoder implemented as a subclassed Layer\n"
      ],
      "metadata": {
        "id": "Xz__JOauLecX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self , embed_dim , dense_dim , num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads \n",
        "    self.attention = layers.MultiHeadAttention(num_heads = num_heads ,\n",
        "                                               key_dim = embed_dim)\n",
        "    self.dense_proj = keras.Sequential([\n",
        "        layers.Dense(dense_dim,activation = \"relu\"),\n",
        "        layers.Dense(embed_dim,)\n",
        "    ])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs , mask=None):\n",
        "    if mask is not None :\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(inputs , \n",
        "                                      inputs,\n",
        "                                      attention_mask = mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\" : self.embed_dim ,\n",
        "        \"num_heads\" : self.num_heads ,\n",
        "        \"dense_dim\" : self.dense_dim,\n",
        "    })\n",
        "    return config \n"
      ],
      "metadata": {
        "id": "6dEK6aNpOZ19"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Transformer encoder for text classification\n",
        "\n",
        "vocab_size = 10000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size , embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim , dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs , outputs )\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9zQ9peIO0Lh",
        "outputId": "07a486d8-f55f-45db-8e58-8e00de200c83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, None, 256)        543776    \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,104,033\n",
            "Trainable params: 3,104,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20,callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUBCcECkPrYI",
        "outputId": "24c55a81-0b64-44d0-b976-b307c3a6f5e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "501/501 [==============================] - 31s 46ms/step - loss: 0.5542 - accuracy: 0.7353 - val_loss: 0.4556 - val_accuracy: 0.7733\n",
            "Epoch 2/20\n",
            "501/501 [==============================] - 13s 26ms/step - loss: 0.3430 - accuracy: 0.8542 - val_loss: 0.3790 - val_accuracy: 0.8298\n",
            "Epoch 3/20\n",
            "501/501 [==============================] - 14s 27ms/step - loss: 0.2624 - accuracy: 0.8950 - val_loss: 0.3724 - val_accuracy: 0.8386\n",
            "Epoch 4/20\n",
            "501/501 [==============================] - 14s 27ms/step - loss: 0.2110 - accuracy: 0.9142 - val_loss: 0.3784 - val_accuracy: 0.8502\n",
            "Epoch 5/20\n",
            "501/501 [==============================] - 13s 27ms/step - loss: 0.1790 - accuracy: 0.9315 - val_loss: 0.4492 - val_accuracy: 0.8332\n",
            "Epoch 6/20\n",
            "501/501 [==============================] - 14s 27ms/step - loss: 0.1534 - accuracy: 0.9430 - val_loss: 0.4297 - val_accuracy: 0.8424\n",
            "Epoch 7/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.1304 - accuracy: 0.9520 - val_loss: 0.4580 - val_accuracy: 0.8347\n",
            "Epoch 8/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.1181 - accuracy: 0.9584 - val_loss: 0.4647 - val_accuracy: 0.8405\n",
            "Epoch 9/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.1034 - accuracy: 0.9640 - val_loss: 0.5022 - val_accuracy: 0.8291\n",
            "Epoch 10/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.0887 - accuracy: 0.9690 - val_loss: 0.5094 - val_accuracy: 0.8321\n",
            "Epoch 11/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.0787 - accuracy: 0.9735 - val_loss: 0.6031 - val_accuracy: 0.8347\n",
            "Epoch 12/20\n",
            "501/501 [==============================] - 13s 26ms/step - loss: 0.0741 - accuracy: 0.9749 - val_loss: 0.6065 - val_accuracy: 0.8257\n",
            "Epoch 13/20\n",
            "501/501 [==============================] - 13s 26ms/step - loss: 0.0686 - accuracy: 0.9772 - val_loss: 0.6329 - val_accuracy: 0.8276\n",
            "Epoch 14/20\n",
            "501/501 [==============================] - 13s 26ms/step - loss: 0.0584 - accuracy: 0.9816 - val_loss: 0.6474 - val_accuracy: 0.8326\n",
            "Epoch 15/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.0508 - accuracy: 0.9850 - val_loss: 0.7149 - val_accuracy: 0.8272\n",
            "Epoch 16/20\n",
            "501/501 [==============================] - 13s 27ms/step - loss: 0.0479 - accuracy: 0.9857 - val_loss: 0.7642 - val_accuracy: 0.8240\n",
            "Epoch 17/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.0454 - accuracy: 0.9848 - val_loss: 0.7860 - val_accuracy: 0.8268\n",
            "Epoch 18/20\n",
            "501/501 [==============================] - 12s 25ms/step - loss: 0.0406 - accuracy: 0.9870 - val_loss: 0.8444 - val_accuracy: 0.8216\n",
            "Epoch 19/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.0350 - accuracy: 0.9900 - val_loss: 0.8749 - val_accuracy: 0.8214\n",
            "Epoch 20/20\n",
            "501/501 [==============================] - 12s 24ms/step - loss: 0.0341 - accuracy: 0.9905 - val_loss: 0.8782 - val_accuracy: 0.8224\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f57044bd220>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"transformer_encoder.keras\",\n",
        "                                custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub51r665SNtd",
        "outputId": "88ee2d70-a406-488f-996f-d946cc1268b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167/167 [==============================] - 2s 10ms/step - loss: 0.3689 - accuracy: 0.8426\n",
            "Test acc: 0.843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length , input_dim , output_dim , **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim ,\n",
        "                                             output_dim = output_dim)\n",
        "    self.position_embeddings = layers.Embedding(input_dim = sequence_length,\n",
        "                                                output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim \n",
        "\n",
        "  def call( self, inputs) :\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def conmpute_mask(self, inputs , mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\" : self.output_dim ,\n",
        "        \"sequence_length\" :  self.sequence_length,\n",
        "        \"input_dim\" : self.input_dim\n",
        "    })\n",
        "    return config "
      ],
      "metadata": {
        "id": "KeV6fWHwSgFd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the Transformer encoder with positional embedding\n",
        "tf.keras.backend.clear_session()\n",
        "vocab_size = 10000\n",
        "sequence_length = 300\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HbtvBQuUTDz",
        "outputId": "57085251-326a-4a2f-9d12-b130c296408f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posit  (None, None, 256)        2636800   \n",
            " ionalEmbedding)                                                 \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, None, 256)        543776    \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,180,833\n",
            "Trainable params: 3,180,833\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20,callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVZwUCTXUrjW",
        "outputId": "b464185c-25a0-4a67-db3f-a38a73ebc1d8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "668/668 [==============================] - 27s 36ms/step - loss: 0.5490 - accuracy: 0.7242 - val_loss: 0.4177 - val_accuracy: 0.8165\n",
            "Epoch 2/20\n",
            "668/668 [==============================] - 18s 27ms/step - loss: 0.3474 - accuracy: 0.8476 - val_loss: 0.3589 - val_accuracy: 0.8370\n",
            "Epoch 3/20\n",
            "668/668 [==============================] - 16s 24ms/step - loss: 0.2594 - accuracy: 0.8952 - val_loss: 0.3410 - val_accuracy: 0.8531\n",
            "Epoch 4/20\n",
            "668/668 [==============================] - 16s 24ms/step - loss: 0.1922 - accuracy: 0.9253 - val_loss: 0.3437 - val_accuracy: 0.8568\n",
            "Epoch 5/20\n",
            "668/668 [==============================] - 16s 24ms/step - loss: 0.1437 - accuracy: 0.9446 - val_loss: 0.4022 - val_accuracy: 0.8506\n",
            "Epoch 6/20\n",
            "668/668 [==============================] - 17s 26ms/step - loss: 0.1038 - accuracy: 0.9629 - val_loss: 0.3963 - val_accuracy: 0.8604\n",
            "Epoch 7/20\n",
            "668/668 [==============================] - 17s 26ms/step - loss: 0.0741 - accuracy: 0.9737 - val_loss: 0.4496 - val_accuracy: 0.8514\n",
            "Epoch 8/20\n",
            "668/668 [==============================] - 17s 26ms/step - loss: 0.0538 - accuracy: 0.9818 - val_loss: 0.4913 - val_accuracy: 0.8560\n",
            "Epoch 9/20\n",
            "668/668 [==============================] - 16s 25ms/step - loss: 0.0382 - accuracy: 0.9864 - val_loss: 0.6516 - val_accuracy: 0.8392\n",
            "Epoch 10/20\n",
            "668/668 [==============================] - 17s 26ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.6514 - val_accuracy: 0.8433\n",
            "Epoch 11/20\n",
            "668/668 [==============================] - 17s 25ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.7305 - val_accuracy: 0.8416\n",
            "Epoch 12/20\n",
            "668/668 [==============================] - 17s 26ms/step - loss: 0.0190 - accuracy: 0.9932 - val_loss: 0.8242 - val_accuracy: 0.8353\n",
            "Epoch 13/20\n",
            "668/668 [==============================] - 16s 24ms/step - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.8256 - val_accuracy: 0.8414\n",
            "Epoch 14/20\n",
            "668/668 [==============================] - 16s 25ms/step - loss: 0.0130 - accuracy: 0.9960 - val_loss: 0.9334 - val_accuracy: 0.8276\n",
            "Epoch 15/20\n",
            "668/668 [==============================] - 16s 24ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.9827 - val_accuracy: 0.8326\n",
            "Epoch 16/20\n",
            "668/668 [==============================] - 17s 26ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 1.0284 - val_accuracy: 0.8371\n",
            "Epoch 17/20\n",
            "668/668 [==============================] - 18s 26ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 1.0141 - val_accuracy: 0.8349\n",
            "Epoch 18/20\n",
            "668/668 [==============================] - 16s 25ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.9761 - val_accuracy: 0.8405\n",
            "Epoch 19/20\n",
            "668/668 [==============================] - 18s 26ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 1.1779 - val_accuracy: 0.8302\n",
            "Epoch 20/20\n",
            "668/668 [==============================] - 17s 26ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 1.1198 - val_accuracy: 0.8407\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f567dde72e0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"full_transformer_encoder.keras\",custom_objects={\"TransformerEncoder\": TransformerEncoder,\"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAUAUiPzYZ-z",
        "outputId": "7a5c0dc2-d825-4e03-d3d3-417d354686ad"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167/167 [==============================] - 2s 11ms/step - loss: 0.3410 - accuracy: 0.8531\n",
            "Test acc: 0.853\n"
          ]
        }
      ]
    }
  ]
}